{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble\n",
    "* 모델 앙상블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로딩 중: vit_final_preds.npy, effnet_final_preds.npy, conv_final_preds.npy\n",
      "20:30:30:20 가중치로 앙상블 중...\n",
      "✅ 최종 앙상블 제출 파일 'submission_ENSEMBLE_effb415_effb515_vit50_conv20.csv' 생성 완료!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"로딩 중: vit_final_preds.npy, effnet_final_preds.npy, conv_final_preds.npy\")\n",
    "vit_preds = np.load(\"npy/vit_final_preds.npy\")\n",
    "eff_b4_preds = np.load(\"npy/eff_b4_final_preds.npy\")\n",
    "eff_b5_preds = np.load(\"npy/eff_b5_final_preds.npy\")\n",
    "conv_preds = np.load(\"npy/conv_final_preds.npy\")\n",
    "\n",
    "# --- 최종 앙상블 ---\n",
    "print(\"20:30:30:20 가중치로 앙상블 중...\")\n",
    "final_avg_preds = (eff_b4_preds * 0.15) + (eff_b5_preds * 0.15) + (vit_preds * 0.5) + (conv_preds * 0.2)\n",
    "\n",
    "# 최종 라벨\n",
    "final_labels = np.argmax(final_avg_preds, axis=1)\n",
    "\n",
    "# 제출 파일 생성\n",
    "tst_df = pd.read_csv(\"datasets_fin/sample_submission.csv\")\n",
    "submission_df = tst_df.copy()\n",
    "submission_df['target'] = final_labels\n",
    "\n",
    "submission_df.to_csv(\"submission/submission_ENSEMBLE_effb415_effb515_vit50_conv20.csv\", index=False)\n",
    "print(\"✅ 최종 앙상블 제출 파일 'submission_ENSEMBLE_effb415_effb515_vit50_conv20.csv' 생성 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR\n",
    "* easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting easyocr\n",
      "  Obtaining dependency information for easyocr from https://files.pythonhosted.org/packages/bb/84/4a2cab0e6adde6a85e7ba543862e5fc0250c51f3ac721a078a55cdcff250/easyocr-1.7.2-py3-none-any.whl.metadata\n",
      "  Downloading easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from easyocr) (2.1.0)\n",
      "Requirement already satisfied: torchvision>=0.5 in /opt/conda/lib/python3.10/site-packages (from easyocr) (0.16.0)\n",
      "Requirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.10/site-packages (from easyocr) (4.8.1.78)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from easyocr) (1.11.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from easyocr) (1.26.0)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from easyocr) (9.4.0)\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from easyocr) (0.22.0)\n",
      "Collecting python-bidi (from easyocr)\n",
      "  Obtaining dependency information for python-bidi from https://files.pythonhosted.org/packages/d3/30/0753601fdad405e806c89cfa9603ff75241f8c7196cfe2cb37c43e34cdbd/python_bidi-0.6.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading python_bidi-0.6.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from easyocr) (6.0)\n",
      "Collecting Shapely (from easyocr)\n",
      "  Obtaining dependency information for Shapely from https://files.pythonhosted.org/packages/50/8a/0ab1f7433a2a85d9e9aea5b1fbb333f3b09b309e7817309250b4b7b2cc7a/shapely-2.1.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Downloading shapely-2.1.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting pyclipper (from easyocr)\n",
      "  Obtaining dependency information for pyclipper from https://files.pythonhosted.org/packages/84/a4/3e304f6c0d000382cd54d4a1e5f0d8fc28e1ae97413a2ec1016a7b840319/pyclipper-1.3.0.post6-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata\n",
      "  Downloading pyclipper-1.3.0.post6-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting ninja (from easyocr)\n",
      "  Obtaining dependency information for ninja from https://files.pythonhosted.org/packages/ed/de/0e6edf44d6a04dabd0318a519125ed0415ce437ad5a1ec9b9be03d9048cf/ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5->easyocr) (2.31.0)\n",
      "Requirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->easyocr) (3.1)\n",
      "Requirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image->easyocr) (2.33.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image->easyocr) (2023.12.9)\n",
      "Requirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image->easyocr) (23.1)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /opt/conda/lib/python3.10/site-packages (from scikit-image->easyocr) (0.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->easyocr) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->easyocr) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->easyocr) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->easyocr) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->easyocr) (2023.9.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->easyocr) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5->easyocr) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5->easyocr) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5->easyocr) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5->easyocr) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->easyocr) (1.3.0)\n",
      "Downloading easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyclipper-1.3.0.post6-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (912 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m912.2/912.2 kB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_bidi-0.6.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shapely-2.1.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: python-bidi, pyclipper, Shapely, ninja, easyocr\n",
      "Successfully installed Shapely-2.1.2 easyocr-1.7.2 ninja-1.13.0 pyclipper-1.3.0.post6 python-bidi-0.6.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install easyocr\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.1% Complete--- 학습 이미지 (train.csv) 1570개 텍스트 추출 중 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1570/1570 [16:21<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'train_ocr.csv' 저장 완료!\n",
      "--- 테스트 이미지 (tst_df) 3140개 텍스트 추출 중 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3140/3140 [08:49<00:00,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'test_ocr.csv' 저장 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. EasyOCR 리더기 초기화 (한국어, 영어)\n",
    "reader = easyocr.Reader(['ko', 'en'])\n",
    "\n",
    "# 2. 텍스트를 추출할 원본 CSV 로드\n",
    "try:\n",
    "    all_df = pd.read_csv(\"datasets_fin/train.csv\") # (1570개)\n",
    "    tst_df = pd.read_csv(\"datasets_fin/sample_submission.csv\") # (3140개)\n",
    "except FileNotFoundError:\n",
    "    print(\"오류: train.csv 또는 sample_submission.csv 파일을 찾을 수 없습니다.\")\n",
    "\n",
    "ocr_results = []\n",
    "img_dir = \"datasets_fin/train\"\n",
    "print(f\"--- 학습 이미지 (train.csv) {len(all_df)}개 텍스트 추출 중 ---\")\n",
    "\n",
    "# 3. 학습 이미지(1570개)에서 텍스트 추출\n",
    "for idx, row in tqdm(all_df.iterrows(), total=len(all_df)):\n",
    "    img_path = os.path.join(img_dir, row['ID'])\n",
    "    try:\n",
    "        # OCR 실행\n",
    "        result = reader.readtext(img_path, detail=0) # detail=0은 텍스트만 추출\n",
    "        extracted_text = ' '.join(result) # 추출된 텍스트 조각들을 하나로 합침\n",
    "        ocr_results.append({'ID': row['ID'], 'target': row['target'], 'text': extracted_text})\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생 ({row['ID']}): {e}\")\n",
    "        ocr_results.append({'ID': row['ID'], 'target': row['target'], 'text': ''}) # 실패 시 빈 텍스트\n",
    "\n",
    "# 4. train_ocr.csv로 저장\n",
    "ocr_train_df = pd.DataFrame(ocr_results)\n",
    "ocr_train_df.to_csv(\"datasets_fin/train_ocr.csv\", index=False)\n",
    "print(\"✅ 'train_ocr.csv' 저장 완료!\")\n",
    "\n",
    "\n",
    "# 5. 테스트 이미지(3140개)에서 텍스트 추출\n",
    "ocr_results = []\n",
    "img_dir = \"datasets_fin/test\"\n",
    "print(f\"--- 테스트 이미지 (tst_df) {len(tst_df)}개 텍스트 추출 중 ---\")\n",
    "\n",
    "for idx, row in tqdm(tst_df.iterrows(), total=len(tst_df)):\n",
    "    img_path = os.path.join(img_dir, row['ID'])\n",
    "    try:\n",
    "        result = reader.readtext(img_path, detail=0)\n",
    "        extracted_text = ' '.join(result)\n",
    "        ocr_results.append({'ID': row['ID'], 'target': row['target'], 'text': extracted_text})\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생 ({row['ID']}): {e}\")\n",
    "        ocr_results.append({'ID': row['ID'], 'target': row['target'], 'text': ''})\n",
    "\n",
    "# 6. test_ocr.csv로 저장\n",
    "ocr_test_df = pd.DataFrame(ocr_results)\n",
    "ocr_test_df.to_csv(\"datasets_fin/test_ocr.csv\", index=False)\n",
    "print(\"✅ 'test_ocr.csv' 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 텍스트 모델(Tfidf + LR) 학습 및 검증 시작 ---\n",
      "텍스트 학습: 1256개, 텍스트 홀드아웃: 314개\n",
      "Tfidf 벡터화 완료.\n",
      "텍스트 모델 학습 완료.\n",
      "--- 텍스트 모델 '미니 리더보드' F1 점수: 0.8866 ---\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"--- 텍스트 모델(Tfidf + LR) 학습 및 검증 시작 ---\")\n",
    "\n",
    "# 1. 텍스트 데이터 로드\n",
    "try:\n",
    "    ocr_train_df = pd.read_csv(\"datasets_fin/train_ocr.csv\")\n",
    "    # 'text' 컬럼이 비어있는(NaN) 경우, 빈 문자열로 채웁니다.\n",
    "    ocr_train_df['text'] = ocr_train_df['text'].fillna('')\n",
    "except FileNotFoundError:\n",
    "    print(\"오류: 2단계에서 생성된 'train_ocr.csv' 파일을 찾을 수 없습니다.\")\n",
    "\n",
    "# 2. [✅ 핵심] 이미지 모델과 *똑같은* '미니 리더보드' 분리\n",
    "# (random_state=42를 사용하여 'holdout_df'가 이미지 모델과 동일하게 분리됨)\n",
    "text_train_df, text_holdout_df = train_test_split(\n",
    "    ocr_train_df, \n",
    "    test_size=0.2,    \n",
    "    random_state=42,  # <--- 이미지 모델과 동일한 random_state\n",
    "    stratify=ocr_train_df['target'] \n",
    ")\n",
    "print(f\"텍스트 학습: {len(text_train_df)}개, 텍스트 홀드아웃: {len(text_holdout_df)}개\")\n",
    "\n",
    "# 3. 텍스트 특징 추출기(Tfidf) 정의 및 학습\n",
    "# (단어/글자 단위(ngram)로 특징을 잡습니다)\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='char_wb', # 단어(word boundary) 사이의 글자(char)\n",
    "    ngram_range=(2, 5), # 2글자 ~ 5글자 조합을 모두 특징으로 사용\n",
    "    max_features=20000  # 최대 특징 개수\n",
    ")\n",
    "\n",
    "# 학습 텍스트(1256개)로 Tfidf 학습\n",
    "X_train_text = vectorizer.fit_transform(text_train_df['text'])\n",
    "y_train_text = text_train_df['target']\n",
    "print(\"Tfidf 벡터화 완료.\")\n",
    "\n",
    "# 4. 텍스트 분류기(LogisticRegression) 학습\n",
    "text_model = LogisticRegression(solver='liblinear', random_state=42)\n",
    "text_model.fit(X_train_text, y_train_text)\n",
    "print(\"텍스트 모델 학습 완료.\")\n",
    "\n",
    "# 5. [✅] 텍스트 모델 '미니 리더보드' 점수 확인\n",
    "X_holdout_text = vectorizer.transform(text_holdout_df['text'])\n",
    "y_holdout_true = text_holdout_df['target']\n",
    "y_holdout_pred_text = text_model.predict(X_holdout_text)\n",
    "\n",
    "text_f1_score = f1_score(y_holdout_true, y_holdout_pred_text, average='macro')\n",
    "print(f\"--- 텍스트 모델 '미니 리더보드' F1 점수: {text_f1_score:.4f} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 최종 4-모델 앙상블 (Image*3 + Text*1) 시작 ---\n",
      "1. 텍스트 모델(.npy) 생성 중...\n",
      "✅ 'text_final_preds.npy' 저장 완료.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"--- 최종 4-모델 앙상블 (Image*3 + Text*1) 시작 ---\")\n",
    "\n",
    "# --- (A) 텍스트 모델 예측 확률 생성 ---\n",
    "print(\"1. 텍스트 모델(.npy) 생성 중...\")\n",
    "try:\n",
    "    ocr_test_df = pd.read_csv(\"datasets_fin/test_ocr.csv\")\n",
    "    ocr_test_df['text'] = ocr_test_df['text'].fillna('')\n",
    "    \n",
    "    # [✅] '진짜 테스트 셋' 텍스트를 벡터화\n",
    "    X_test_text = vectorizer.transform(ocr_test_df['text'])\n",
    "    \n",
    "    # [✅] '진짜 테스트 셋'의 확률(predict_proba) 예측\n",
    "    text_model_probs = text_model.predict_proba(X_test_text) # (3140, 17)\n",
    "    \n",
    "    # 4번째 앙상블 멤버로 저장\n",
    "    np.save(\"npy/text_final_preds.npy\", text_model_probs)\n",
    "    print(\"✅ 'text_final_preds.npy' 저장 완료.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"오류: 텍스트 모델 예측 중 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. 이미지 3개 + 텍스트 1개 앙상블 블렌딩...\n",
      "✅ 최종 멀티모달 앙상블 제출 파일 'submission_MULTIMODAL_FINAL2.csv' 생성 완료!\n"
     ]
    }
   ],
   "source": [
    "# --- (B) 3-모델 이미지 앙상블 + 텍스트 앙상블 ---\n",
    "print(\"2. 이미지 3개 + 텍스트 1개 앙상블 블렌딩...\")\n",
    "try:\n",
    "    # 1. 기존 이미지 모델 3개의 확률 로드\n",
    "    vit_preds = np.load(\"npy/vit_final_preds.npy\")\n",
    "    effnet_preds = np.load(\"npy/effnet_final_preds.npy\")\n",
    "    convnext_preds = np.load(\"npy/conv_final_preds.npy\")\n",
    "    \n",
    "    # 2. 방금 생성한 텍스트 모델 확률 로드\n",
    "    text_preds = np.load(\"npy/text_final_preds.npy\")\n",
    "\n",
    "    # 3. [✅ 핵심] 가중치 조절\n",
    "    # 예시: 0.9069점을 만든 이미지 앙상블(가중치 70%) + 텍스트 모델(가중치 30%)\n",
    "    \n",
    "    # 3-1. 이미지 앙상블 (0.9069점짜리)\n",
    "    image_ensemble_probs = (convnext_preds * 0.4) + (effnet_preds * 0.4) + (vit_preds * 0.2)\n",
    "    \n",
    "    # 3-2. 이미지(70%) + 텍스트(30%) 최종 앙상블\n",
    "    final_avg_preds = (image_ensemble_probs * 0.95) + (text_preds * 0.05)\n",
    "    \n",
    "    # (팁: 이 0.7, 0.3 가중치를 0.6:0.4, 0.8:0.2 등으로 조절하며 최고 점수를 찾아야 합니다)\n",
    "\n",
    "    # 4. 최종 라벨\n",
    "    final_labels = np.argmax(final_avg_preds, axis=1)\n",
    "\n",
    "    # 5. 제출 파일 생성\n",
    "    tst_df = pd.read_csv(\"datasets_fin/sample_submission.csv\")\n",
    "    submission_df = tst_df.copy()\n",
    "    submission_df['target'] = final_labels\n",
    "\n",
    "    submission_df.to_csv(\"submission/submission_MULTIMODAL_FINAL2.csv\", index=False)\n",
    "    print(\"✅ 최종 멀티모달 앙상블 제출 파일 'submission_MULTIMODAL_FINAL2.csv' 생성 완료!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"오류: .npy 파일 (vit, effnet, convnext) 중 일부를 찾을 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* KLUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[sentencepiece] in /opt/conda/lib/python3.10/site-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (2025.11.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (4.65.0)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.2.1)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (6.33.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[sentencepiece]) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[sentencepiece]) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[sentencepiece]) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[sentencepiece]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[sentencepiece]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[sentencepiece]) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[sentencepiece]) (2023.7.22)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ KLUE/roberta-base 토크나이저 및 데이터셋 클래스 정의 완료.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# [✅ 교체] BertTokenizerFast 대신, 'AutoTokenizer'를 사용합니다\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification \n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# --- (A) KLUE 모델 및 토크나이저 정의 (표준 방식) ---\n",
    "KLUE_MODEL_NAME = 'klue/roberta-base'\n",
    "\n",
    "# 'AutoTokenizer'가 KLUE 모델을 자동으로 인식하고 로드합니다\n",
    "tokenizer = AutoTokenizer.from_pretrained(KLUE_MODEL_NAME) \n",
    "\n",
    "# --- (B) KoBERT용 데이터셋 클래스 (이름만 KoBERT, 내용은 동일) ---\n",
    "class KoBERTDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe['text'].values\n",
    "        self.targets = dataframe['target'].values\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        target = self.targets[index]\n",
    "\n",
    "        # KLUE 토크나이저로 텍스트 인코딩 (KoBERT와 동일)\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True, \n",
    "            max_length=self.max_len,\n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True, \n",
    "            return_tensors='pt' \n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"✅ KLUE/roberta-base 토크나이저 및 데이터셋 클래스 정의 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 텍스트 모델(KLUE) '미니 리더보드' 학습 시작 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Train] Loss: 1.1281: 100%|██████████| 79/79 [00:07<00:00, 11.01it/s]\n",
      "Epoch 1/20 [Valid]: 100%|██████████| 20/20 [00:00<00:00, 33.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 텍스트 모델(KLUE) '미니 리더보드' F1 점수: 0.5084 ---\n",
      "⭐️ KLUE Best F1 Score updated to 0.5084. Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [Train] Loss: 0.9312: 100%|██████████| 79/79 [00:07<00:00, 11.02it/s]\n",
      "Epoch 2/20 [Valid]: 100%|██████████| 20/20 [00:00<00:00, 33.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 텍스트 모델(KLUE) '미니 리더보드' F1 점수: 0.7721 ---\n",
      "⭐️ KLUE Best F1 Score updated to 0.7721. Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [Train] Loss: 0.4001: 100%|██████████| 79/79 [00:07<00:00, 11.02it/s]\n",
      "Epoch 3/20 [Valid]: 100%|██████████| 20/20 [00:00<00:00, 33.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 텍스트 모델(KLUE) '미니 리더보드' F1 점수: 0.8518 ---\n",
      "⭐️ KLUE Best F1 Score updated to 0.8518. Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 [Train] Loss: 0.1172: 100%|██████████| 79/79 [00:07<00:00, 10.95it/s]\n",
      "Epoch 4/20 [Valid]: 100%|██████████| 20/20 [00:00<00:00, 32.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 텍스트 모델(KLUE) '미니 리더보드' F1 점수: 0.9113 ---\n",
      "⭐️ KLUE Best F1 Score updated to 0.9113. Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 [Train] Loss: 0.0731: 100%|██████████| 79/79 [00:07<00:00, 10.88it/s]\n",
      "Epoch 5/20 [Valid]: 100%|██████████| 20/20 [00:00<00:00, 33.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 텍스트 모델(KLUE) '미니 리더보드' F1 점수: 0.9046 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 [Train] Loss: 0.0628: 100%|██████████| 79/79 [00:07<00:00, 10.90it/s]\n",
      "Epoch 6/20 [Valid]: 100%|██████████| 20/20 [00:00<00:00, 33.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 텍스트 모델(KLUE) '미니 리더보드' F1 점수: 0.9181 ---\n",
      "⭐️ KLUE Best F1 Score updated to 0.9181. Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 [Train] Loss: 0.0882: 100%|██████████| 79/79 [00:07<00:00, 10.99it/s]\n",
      "Epoch 7/20 [Valid]: 100%|██████████| 20/20 [00:00<00:00, 33.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 텍스트 모델(KLUE) '미니 리더보드' F1 점수: 0.9303 ---\n",
      "⭐️ KLUE Best F1 Score updated to 0.9303. Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 [Train] Loss: 0.0875: 100%|██████████| 79/79 [00:07<00:00, 10.97it/s]\n",
      "Epoch 8/20 [Valid]: 100%|██████████| 20/20 [00:00<00:00, 33.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 텍스트 모델(KLUE) '미니 리더보드' F1 점수: 0.9190 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 [Train] Loss: 0.0394: 100%|██████████| 79/79 [00:07<00:00, 10.97it/s]\n",
      "Epoch 9/20 [Valid]: 100%|██████████| 20/20 [00:00<00:00, 32.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 텍스트 모델(KLUE) '미니 리더보드' F1 점수: 0.9254 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 [Train] Loss: 0.0618: 100%|██████████| 79/79 [00:07<00:00, 10.96it/s]\n",
      "Epoch 10/20 [Valid]: 100%|██████████| 20/20 [00:00<00:00, 33.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 텍스트 모델(KLUE) '미니 리더보드' F1 점수: 0.9408 ---\n",
      "⭐️ KLUE Best F1 Score updated to 0.9408. Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 [Train] Loss: 0.0197: 100%|██████████| 79/79 [00:07<00:00, 10.95it/s]\n",
      "Epoch 11/20 [Valid]: 100%|██████████| 20/20 [00:00<00:00, 33.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 텍스트 모델(KLUE) '미니 리더보드' F1 점수: 0.9436 ---\n",
      "⭐️ KLUE Best F1 Score updated to 0.9436. Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 [Train] Loss: 0.0341: 100%|██████████| 79/79 [00:07<00:00, 10.94it/s]\n",
      "Epoch 12/20 [Valid]: 100%|██████████| 20/20 [00:00<00:00, 33.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 텍스트 모델(KLUE) '미니 리더보드' F1 점수: 0.9509 ---\n",
      "⭐️ KLUE Best F1 Score updated to 0.9509. Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 [Train] Loss: 0.0146: 100%|██████████| 79/79 [00:07<00:00, 10.94it/s]\n",
      "Epoch 13/20 [Valid]: 100%|██████████| 20/20 [00:00<00:00, 33.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 텍스트 모델(KLUE) '미니 리더보드' F1 점수: 0.9456 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 [Train] Loss: 0.0226: 100%|██████████| 79/79 [00:07<00:00, 10.95it/s]\n",
      "Epoch 14/20 [Valid]: 100%|██████████| 20/20 [00:00<00:00, 32.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 텍스트 모델(KLUE) '미니 리더보드' F1 점수: 0.9292 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 [Train] Loss: 0.0104: 100%|██████████| 79/79 [00:07<00:00, 10.91it/s]\n",
      "Epoch 15/20 [Valid]: 100%|██████████| 20/20 [00:00<00:00, 33.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 텍스트 모델(KLUE) '미니 리더보드' F1 점수: 0.9416 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 [Train] Loss: 0.0110: 100%|██████████| 79/79 [00:07<00:00, 10.94it/s]\n",
      "Epoch 16/20 [Valid]: 100%|██████████| 20/20 [00:00<00:00, 33.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 텍스트 모델(KLUE) '미니 리더보드' F1 점수: 0.9455 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 [Train] Loss: 0.0086: 100%|██████████| 79/79 [00:07<00:00, 10.94it/s]\n",
      "Epoch 17/20 [Valid]: 100%|██████████| 20/20 [00:00<00:00, 33.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 텍스트 모델(KLUE) '미니 리더보드' F1 점수: 0.9398 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 [Train] Loss: 0.1560: 100%|██████████| 79/79 [00:07<00:00, 10.93it/s]\n",
      "Epoch 18/20 [Valid]: 100%|██████████| 20/20 [00:00<00:00, 33.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 텍스트 모델(KLUE) '미니 리더보드' F1 점수: 0.9276 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 [Train] Loss: 0.0073: 100%|██████████| 79/79 [00:07<00:00, 10.93it/s]\n",
      "Epoch 19/20 [Valid]: 100%|██████████| 20/20 [00:00<00:00, 33.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 텍스트 모델(KLUE) '미니 리더보드' F1 점수: 0.9313 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 [Train] Loss: 0.0056: 100%|██████████| 79/79 [00:07<00:00, 10.92it/s]\n",
      "Epoch 20/20 [Valid]: 100%|██████████| 20/20 [00:00<00:00, 33.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 텍스트 모델(KLUE) '미니 리더보드' F1 점수: 0.9445 ---\n",
      "--- KLUE 학습 완료! 최종 '미니 리더보드' 점수: 0.9509 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- (A) 하이퍼파라미터 (KoBERT와 동일) ---\n",
    "MAX_LEN = 128      \n",
    "BATCH_SIZE = 16    \n",
    "EPOCHS = 20         \n",
    "LR = 3e-5          \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# --- (B) 데이터로더 생성 (KoBERT 2단계에서 만든 변수 사용) ---\n",
    "# (text_train_df, text_holdout_df 변수가 살아있어야 합니다)\n",
    "train_dataset = KoBERTDataset(text_train_df, tokenizer, MAX_LEN)\n",
    "holdout_dataset = KoBERTDataset(text_holdout_df, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "holdout_loader = DataLoader(holdout_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# --- (C) 모델 및 옵티마이저 ---\n",
    "# [✅ 교체] 'AutoModel'이 KLUE 모델을 자동으로 로드합니다\n",
    "model = AutoModelForSequenceClassification.from_pretrained(KLUE_MODEL_NAME, num_labels=17).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "loss_fn = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "\n",
    "print(\"--- 텍스트 모델(KLUE) '미니 리더보드' 학습 시작 ---\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{EPOCHS} [Train]\")\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=targets)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_description(f\"Epoch {epoch + 1}/{EPOCHS} [Train] Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # --- [✅ 핵심 검증] '미니 리더보드' F1 점수 확인 ---\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(holdout_loader, desc=f\"Epoch {epoch + 1}/{EPOCHS} [Valid]\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    val_f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "    print(f\"--- 텍스트 모델(KLUE) '미니 리더보드' F1 점수: {val_f1:.4f} ---\")\n",
    "\n",
    "    # [✅] 0.91+ 점을 넘어야 앙상블에 기여 가능!\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(model.state_dict(), \"model/klue_best_model.pth\")\n",
    "        print(f\"⭐️ KLUE Best F1 Score updated to {val_f1:.4f}. Model saved!\")\n",
    "\n",
    "print(f\"--- KLUE 학습 완료! 최종 '미니 리더보드' 점수: {best_val_f1:.4f} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 텍스트 모델(KLUE) '진짜 테스트 데이터' 추론 시작 (NPY 생성) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "KLUE TTA 추론 중: 100%|██████████| 99/99 [00:05<00:00, 19.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ KLUE (Test Data) 확률 저장 완료! (klue_final_preds.npy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"--- 텍스트 모델(KLUE) '진짜 테스트 데이터' 추론 시작 (NPY 생성) ---\")\n",
    "\n",
    "# 1. 테스트 텍스트 로드 (easyocr로 생성한 파일)\n",
    "ocr_test_df = pd.read_csv(\"datasets_fin/test_ocr.csv\").fillna('')\n",
    "test_dataset = KoBERTDataset(ocr_test_df, tokenizer, MAX_LEN)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE * 2, shuffle=False) # 추론은 배치 2배\n",
    "\n",
    "# 2. 저장된 Best KLUE 모델 로드\n",
    "# [✅ KLUE] AutoModel로 로드\n",
    "model = AutoModelForSequenceClassification.from_pretrained(KLUE_MODEL_NAME, num_labels=17).to(device)\n",
    "model.load_state_dict(torch.load(\"model/klue_best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# 3. 추론 및 확률 추출\n",
    "all_probs = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"KLUE TTA 추론 중\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # [✅] 로짓을 확률(softmax)로 변환\n",
    "        probs = torch.softmax(logits, dim=1) \n",
    "        all_probs.append(probs.cpu().numpy())\n",
    "\n",
    "# 4. .npy 파일로 저장\n",
    "klue_final_preds_array = np.concatenate(all_probs, axis=0) # (3140, 17)\n",
    "np.save(\"npy/klue_final_preds.npy\", klue_final_preds_array)\n",
    "print(\"✅ KLUE (Test Data) 확률 저장 완료! (klue_final_preds.npy)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 최종 4-모델 앙상블 (Image*3 + KLUE*1) 시작 ---\n",
      "✅ 최종 멀티모달(KLUE 90:10) 앙상블 제출 파일 'submission_MULTIMODAL_KLUE_100_0.csv' 생성 완료!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"--- 최종 4-모델 앙상블 (Image*3 + KLUE*1) 시작 ---\")\n",
    "\n",
    "try:\n",
    "    # 1. 기존 이미지 모델 3개의 확률 로드\n",
    "    vit_preds = np.load(\"npy/vit_final_preds.npy\")\n",
    "    eff_b5_preds = np.load(\"npy/eff_b5_final_preds.npy\")\n",
    "    convnext_preds = np.load(\"npy/conv_final_preds.npy\")\n",
    "    \n",
    "    # 2. 방금 생성한 KLUE 확률 로드\n",
    "    text_preds = np.load(\"npy/klue_final_preds.npy\")\n",
    "\n",
    "    # 3. [✅ 핵심] 4-모델 가중치\n",
    "    # 이미지 앙상블(0.9069점)과 KLUE(0.9067점)가 거의 동일한 수준이므로,\n",
    "    # 80:20 또는 70:30 가중치를 시도해볼 가치가 있습니다.\n",
    "    \n",
    "    # 3-1. 이미지 앙상블 (0.9069점짜리)\n",
    "    image_ensemble_probs = (eff_b5_preds * 0.2) + (vit_preds * 0.5) + (convnext_preds * 0.3)\n",
    "    \n",
    "    # 3-2. 이미지(80%) + KLUE(20%) 최종 앙상블\n",
    "    final_avg_preds = (image_ensemble_probs * 1.0) + (text_preds * 0)\n",
    "\n",
    "    # 4. 최종 라벨\n",
    "    final_labels = np.argmax(final_avg_preds, axis=1)\n",
    "\n",
    "    # 5. 제출 파일 생성\n",
    "    tst_df = pd.read_csv(\"datasets_fin/sample_submission.csv\")\n",
    "    submission_df = tst_df.copy()\n",
    "    submission_df['target'] = final_labels\n",
    "\n",
    "    submission_df.to_csv(\"submission/submission_MULTIMODAL_KLUE_100_0.csv\", index=False)\n",
    "    print(\"✅ 최종 멀티모달(KLUE 90:10) 앙상블 제출 파일 'submission_MULTIMODAL_KLUE_100_0.csv' 생성 완료!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"오류: .npy 파일 (vit, effnet, convnext, klue) 중 일부를 찾을 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 의사 라벨링 (Pseudo-Labeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1단계: '가짜 정답(Pseudo-Label)' 생성 시작 ---\n",
      "0.9069점 앙상블 가중치로 확률 계산 중...\n",
      "✅ 1022개의 '확신' 샘플(가짜 정답)을 찾았습니다.\n",
      "✅ 'pseudo_label.csv' 파일 저장 완료.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil # (파일 복사를 위해 import)\n",
    "\n",
    "print(\"--- 1단계: '가짜 정답(Pseudo-Label)' 생성 시작 ---\")\n",
    "\n",
    "# 1. 3-모델 .npy 파일 로드\n",
    "try:\n",
    "    vit_preds = np.load(\"npy/vit_final_preds.npy\")\n",
    "    effnet_preds = np.load(\"npy/effnet_final_preds.npy\")\n",
    "    convnext_preds = np.load(\"npy/conv_final_preds.npy\")\n",
    "    tst_df = pd.read_csv(\"datasets_fin/sample_submission.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"오류: 3-모델 .npy 파일 또는 sample_submission.csv가 없습니다.\")\n",
    "\n",
    "# 2. [✅ 핵심] 0.9069점을 만든 '가중치'로 앙상블\n",
    "# (이전에 0.9069점을 얻은 가중치 조합을 사용하세요. 예: B4:30, ViT:40, Conv:30)\n",
    "print(\"0.9069점 앙상블 가중치로 확률 계산 중...\")\n",
    "image_ensemble_probs = (effnet_preds * 0.3) + (vit_preds * 0.4) + (convnext_preds * 0.3)\n",
    "\n",
    "# 3. 예측 라벨 및 '최대 확률' 추출\n",
    "pseudo_labels = np.argmax(image_ensemble_probs, axis=1)\n",
    "max_probs = np.max(image_ensemble_probs, axis=1)\n",
    "\n",
    "# 4. [✅ 핵심] '확증 편향' 방지를 위한 '높은 임계값'\n",
    "CONFIDENCE_THRESHOLD = 0.999 # (99.9% 이상 확신하는 샘플만)\n",
    "\n",
    "# 5. 임계값을 넘는 '확신' 샘플의 인덱스(indices) 추출\n",
    "confident_indices = np.where(max_probs > CONFIDENCE_THRESHOLD)[0]\n",
    "\n",
    "if len(confident_indices) == 0:\n",
    "    print(\"🚨 경고: 99.9% 이상 확신하는 샘플이 0개입니다. THRESHOLD를 0.99로 낮춰보세요.\")\n",
    "else:\n",
    "    print(f\"✅ {len(confident_indices)}개의 '확신' 샘플(가짜 정답)을 찾았습니다.\")\n",
    "\n",
    "# 6. '가짜 정답' DataFrame 생성\n",
    "pseudo_label_df = pd.DataFrame({\n",
    "    'ID': tst_df.iloc[confident_indices]['ID'],\n",
    "    'target': pseudo_labels[confident_indices]\n",
    "})\n",
    "\n",
    "# 7. '가짜 정답' CSV 저장\n",
    "pseudo_label_df.to_csv(\"datasets_fin/pseudo_label.csv\", index=False)\n",
    "print(\"✅ 'pseudo_label.csv' 파일 저장 완료.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
